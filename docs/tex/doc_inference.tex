\section{\texorpdfstring{\texttt{fitr.inference}}{fitr.inference}}\label{fitr.inference}

Methods for inferring the parameters of generative models for
reinforcement learning data.

\subsection{OptimizationResult}\label{optimizationresult}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.inference.optimization_result.OptimizationResult()}
\end{Highlighting}
\end{Shaded}

Container for the results of an optimization run on a generative model
of behavioural data

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{subject\_id}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Integer ids for subjects
\item
  \textbf{xmin}: \texttt{ndarray((nsubjects,nparams))} or \texttt{None}
  (default). Parameters that minimize objective function
\item
  \textbf{fmin}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Value of objective function at minimum
\item
  \textbf{fevals}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Number of function evaluations required to minimize
  objective function
\item
  \textbf{niters}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Number of iterations required to minimize objective
  function
\item
  \textbf{lme}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Log model evidence
\item
  \textbf{bic}: \texttt{ndarray((nsubjects,))} or \texttt{None}
  (default). Bayesian Information Criterion
\item
  \textbf{hess\_inv}: \texttt{ndarray((nsubjects,nparams,nparams))} or
  \texttt{None} (default). Inverse Hessian at the optimum.
\item
  \textbf{err}: \texttt{ndarray((nsubjects,nparams))} or \texttt{None}
  (default). Error of estimates at optimum.
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsubsection{OptimizationResult.transform\_xmin}\label{optimizationresult.transform_xmin}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.inference.optimization_result.transform_xmin(}\VariableTok{self}\NormalTok{, transforms, inplace}\OperatorTok{=}\VariableTok{False}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Rescales the parameter estimates.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{transforms}: \texttt{list}. Transformation functions where
  \texttt{len(transforms)\ ==\ self.xmin.shape{[}1{]}}
\item
  \textbf{inplace}: \texttt{bool}. Whether to change the values in
  \texttt{self.xmin}. Default is \texttt{False}, which returns an
  \texttt{ndarray((nsubjects,\ nparams))} of the transformed parameters.
\end{itemize}

Returns:

\texttt{ndarray((nsubjects,\ nparams))} of the transformed parameters if
\texttt{inplace=False}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{mlepar}\label{mlepar}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.inference.mle_parallel.mlepar(f, data, nparams, minstarts}\OperatorTok{=}\DecValTok{2}\NormalTok{, maxstarts}\OperatorTok{=}\DecValTok{10}\NormalTok{, maxstarts_without_improvement}\OperatorTok{=}\DecValTok{3}\NormalTok{, init_sd}\OperatorTok{=}\DecValTok{2}\NormalTok{, njobs}\OperatorTok{=-}\DecValTok{1}\NormalTok{, jac}\OperatorTok{=}\VariableTok{None}\NormalTok{, hess}\OperatorTok{=}\VariableTok{None}\NormalTok{, method}\OperatorTok{=}\StringTok{'L-BFGS-B'}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Computes maximum likelihood estimates using parallel CPU resources.

Wraps over the \texttt{fitr.optimization.mle\_parallel.mle} function.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{f}: Likelihood function
\item
  \textbf{data}: A subscriptable object whose first dimension indexes
  subjects
\item
  \textbf{optimizer}: Optimization function (currently only
  \texttt{l\_bfgs\_b} supported)
\item
  \textbf{nparams}: \texttt{int} number of parameters to be estimated
\item
  \textbf{minstarts}: \texttt{int}. Minimum number of restarts with new
  initial values
\item
  \textbf{maxstarts}: \texttt{int}. Maximum number of restarts with new
  initial values
\item
  \textbf{maxstarts\_without\_improvement}: \texttt{int}. Maximum number
  of restarts without improvement in objective function value
\item
  \textbf{init\_sd}: Standard deviation for Gaussian initial values
\item
  \textbf{jac}: \texttt{bool}. Set to \texttt{True} if \texttt{f}
  returns a Jacobian as the second element of the returned values
\item
  \textbf{hess}: \texttt{bool}. Set to \texttt{True} if third output
  value of \texttt{f} is the Hessian matrix
\item
  \textbf{method}: \texttt{str}. One of the \texttt{scipy.optimize}
  methods.
\end{itemize}

Returns:

\texttt{fitr.inference.OptimizationResult}

Todo:

\begin{itemize}
\tightlist
\item
  {[} {]} Raise errors when user selects inappropriate optimization
  function given values for \texttt{jac} and \texttt{hess}
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{l\_bfgs\_b}\label{l_bfgs_b}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.inference.mle_parallel.l_bfgs_b(f, i, data, nparams, jac, minstarts}\OperatorTok{=}\DecValTok{2}\NormalTok{, maxstarts}\OperatorTok{=}\DecValTok{10}\NormalTok{, maxstarts_without_improvement}\OperatorTok{=}\DecValTok{3}\NormalTok{, init_sd}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Minimizes the negative log-probability of data with respect to some
parameters under function \texttt{f} using the L-BFGS-B algorithm.

This function is specified for use with parallel CPU resources.

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{f}: Log likelihood function
\item
  \textbf{i}: \texttt{int}. Subject being optimized (slices first
  dimension of \texttt{data})
\item
  \textbf{data}: Object subscriptable along first dimension to indicate
  subject being optimized
\item
  \textbf{nparams}: \texttt{int}. Number of parameters in the model
\item
  \textbf{jac}: \texttt{bool}. Set to \texttt{True} if \texttt{f}
  returns a Jacobian as the second element of the returned values
\item
  \textbf{minstarts}: \texttt{int}. Minimum number of restarts with new
  initial values
\item
  \textbf{maxstarts}: \texttt{int}. Maximum number of restarts with new
  initial values
\item
  \textbf{maxstarts\_without\_improvement}: \texttt{int}. Maximum number
  of restarts without improvement in objective function value
\item
  \textbf{init\_sd}: Standard deviation for Gaussian initial values
\end{itemize}

Returns:

\begin{itemize}
\tightlist
\item
  \textbf{i}: \texttt{int}. Subject being optimized (slices first
  dimension of \texttt{data})
\item
  \textbf{xmin}: \texttt{ndarray((nparams,))}. Parameter values at
  optimum
\item
  \textbf{fmin}: Scalar objective function value at optimum
\item
  \textbf{fevals}: \texttt{int}. Number of function evaluations
\item
  \textbf{niters}: \texttt{int}. Number of iterations
\item
  \textbf{lme\_}: Scalar log-model evidence at optimum
\item
  \textbf{bic\_}: Scalar Bayesian Information Criterion at optimum
\item
  \textbf{hess\_inv}: \texttt{ndarray((nparams,\ nparams))}. Inv at
  optimum
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\subsection{bms}\label{bms}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fitr.inference.bms.bms(L, ftol}\OperatorTok{=}\FloatTok{1e-12}\NormalTok{, nsamples}\OperatorTok{=}\DecValTok{1000000}\NormalTok{, rng}\OperatorTok{=<}\NormalTok{mtrand.RandomState }\BuiltInTok{object}\NormalTok{ at }\BaseNTok{0x7f0a8a22b0d8}\OperatorTok{>}\NormalTok{, verbose}\OperatorTok{=}\VariableTok{True}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Implements variational Bayesian Model Selection as per Rigoux et al.
(2014).

Arguments:

\begin{itemize}
\tightlist
\item
  \textbf{L}: \texttt{ndarray((nsubjects,\ nmodels))}. Log model
  evidence
\item
  \textbf{ftol}: \texttt{float}. Threshold for convergence of prediction
  error
\item
  \textbf{nsamples}: \texttt{int\textgreater{}0}. Number of samples to
  draw from Dirichlet distribution for computation of exceedence
  probabilities
\item
  \textbf{rng}: \texttt{np.random.RandomState}
\item
  \textbf{verbose}: \texttt{bool\ (default=True)}. If \texttt{False}, no
  output provided.
\end{itemize}

Returns: - \textbf{pxp}: \texttt{ndarray(nmodels)}. Protected exceedance
probabilities - \textbf{xp}: \texttt{ndarray(nmodels)}. Exceedance
probabilities - \textbf{bor}: \texttt{ndarray(nmodels)}. Bayesian
Omnibus Risk - \textbf{pe}: \texttt{ndarray(niter)}. Prediction error
time series throughout optimization - \textbf{q\_m}:
\texttt{ndarray((nsubjects,\ nmodels))}. Posterior distribution over
models for each subject - \textbf{alpha}: \texttt{ndarray(nmodels)}.
Posterior estimates of Dirichlet parameters - \textbf{f0}:
\texttt{float}. Free energy of null model - \textbf{f1}: \texttt{float}.
Free energy of alternative model - \textbf{niter}: \texttt{int}. Number
of iterations of posterior optimization

Examples:

Assuming one is given a matrix of (log-) model evidence values
\texttt{L} of type \texttt{ndarray((nsubjects,\ nmodels))},

\begin{verbatim}
from fitr.inference import spm_bms

pxp, xp, bor, q_m, alpha, f0, f1, niter = bms(L)
\end{verbatim}

Todos:

\begin{itemize}
\tightlist
\item
  {[} {]} Add notes on derivation
\end{itemize}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}
